{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from densenet import densenet_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disable GPU\n",
    "# tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "# tf.config.experimental.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngpus = tf.config.experimental.list_physical_devices(\\'GPU\\')\\nif gpus:\\n    try:\\n        # Currently, memory growth needs to be the same across GPUs\\n        for gpu in gpus:\\n              tf.config.experimental.set_memory_growth(gpu, True)\\n        logical_gpus = tf.config.experimental.list_logical_devices(\\'GPU\\')\\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\\n    except RuntimeError as e:\\n        # Memory growth must be set before GPUs have been initialized\\n        print(e)\\n        \\nprint(tf.config.experimental.list_logical_devices(\\'GPU\\'))\\ntf.test.is_gpu_available()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# data\n",
    "rotation_range = 20\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "shear_range = 0\n",
    "zoom_range = 0.5\n",
    "size = (512,512)\n",
    "\n",
    "# model\n",
    "nb_filter = 64\n",
    "growth_rate = 16\n",
    "nb_layers = [6, 12, 24, 16]\n",
    "reduction = 0.5\n",
    "\n",
    "# training\n",
    "lr = 0.01\n",
    "epochs = 200\n",
    "max_patience = 50\n",
    "batch_size = 2\n",
    "\n",
    "# log\n",
    "log_freq = 1\n",
    "models_directory = 'results/models/'\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "identifier = \"{}-growth-{}-densenet\".format(\n",
    "    '-'.join([str(i) for i in nb_layers]),\n",
    "    growth_rate) + date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dir = '../../data/best-artworks-of-all-time/'\n",
    "images_dir = origin_dir + 'images/'\n",
    "x = []\n",
    "y = []\n",
    "i = 0\n",
    "for artist in os.listdir(images_dir):\n",
    "    for filename in glob.glob(images_dir + artist +'/*.jpg'):\n",
    "        x.append(filename)\n",
    "        y.append(i)\n",
    "    i += 1\n",
    "n_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n",
      "van 1000\n"
     ]
    }
   ],
   "source": [
    "# for a new value newValue, compute the new count, new mean, the new M2.\n",
    "# mean accumulates the mean of the entire dataset\n",
    "# M2 aggregates the squared distance from the mean\n",
    "# count aggregates the number of samples seen so far\n",
    "def update(existingAggregate, newValue):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    count += 1 \n",
    "    delta = newValue - mean\n",
    "    mean += delta / count\n",
    "    delta2 = newValue - mean\n",
    "    M2 += delta * delta2\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "# retrieve the mean, variance and sample variance from an aggregate\n",
    "def finalize(existingAggregate):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    (mean, variance, sampleVariance) = (mean, M2/count, M2/(count - 1)) \n",
    "    if count < 2:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return (mean, variance, sampleVariance)\n",
    "\n",
    "size = (512,512)\n",
    " \n",
    "existingAggregate = (0, np.zeros((512,512,3), dtype=np.float64), np.zeros((512,512,3), dtype=np.float64))\n",
    "for i in range(len(x)):\n",
    "    existingAggregate = update(existingAggregate, cv2.resize(cv2.imread(x[i]), size)/255.)\n",
    "    if (i > 0 and i % 800 == 0):\n",
    "        print('van 1000')\n",
    "mean, variance, sampleVariance = finalize(existingAggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y)\n",
    "n_classes = len(classes)\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                     classes,\n",
    "                                     y)\n",
    "y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9688172 , 0.54494048, 1.11646341, 1.36641791, 1.67981651,\n",
       "       0.69885496, 0.97393617, 1.86836735, 0.76610879, 2.12906977,\n",
       "       1.30785714, 2.54305556, 1.94787234, 0.62920962, 1.84949495,\n",
       "       0.55823171, 1.4531746 , 0.71803922, 1.66454545, 0.94870466,\n",
       "       1.30785714, 2.26049383, 1.56495726, 1.07076023, 0.94381443,\n",
       "       0.70694981, 1.53865546, 1.79509804, 1.38712121, 0.41708428,\n",
       "       1.33649635, 1.55169492, 1.25410959, 0.26082621, 1.31726619,\n",
       "       1.36641791, 0.20877993, 1.43046875, 1.28041958, 2.1045977 ,\n",
       "       2.08068182, 2.1797619 , 1.01160221, 0.58874598, 1.52583333,\n",
       "       2.03444444, 0.9844086 , 1.29858156, 2.01208791, 2.26049383])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    rotation_range=rotation_range,\n",
    "    width_shift_range=width_shift_range,\n",
    "    height_shift_range=height_shift_range,\n",
    "    horizontal_flip=horizontal_flip,\n",
    "    vertical_flip = vertical_flip,\n",
    "    shear_range=shear_range,\n",
    "    zoom_range=zoom_range,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimg_input = Input(shape=img_shape, name='data')\\nbase_model = tf.keras.applications.densenet.DenseNet121(include_top=False, weights='imagenet', input_shape=img_shape)\\nbase_model.trainable = False\\nx = base_model(img_input)\\nx = BatchNormalization(name='conv_final_blk_bn')(x)\\nx = Activation('relu', name='relu_final_blk')(x)\\nx = GlobalAveragePooling2D(name='pool_final')(x)\\nx = Dense(n_classes, name='fc6')(x)\\noutput = Activation('softmax', name='prob')(x)\\nmodel = Model(inputs=img_input, outputs=output)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape = cv2.resize(cv2.imread(x[0]), size).shape\n",
    "model = densenet_model(classes=n_classes, nb_filter=nb_filter, shape=img_shape, growth_rate=growth_rate, nb_layers=nb_layers, reduction=reduction)\n",
    "'''\n",
    "img_input = Input(shape=img_shape, name='data')\n",
    "base_model = tf.keras.applications.densenet.DenseNet121(include_top=False, weights='imagenet', input_shape=img_shape)\n",
    "base_model.trainable = False\n",
    "x = base_model(img_input)\n",
    "x = BatchNormalization(name='conv_final_blk_bn')(x)\n",
    "x = Activation('relu', name='relu_final_blk')(x)\n",
    "x = GlobalAveragePooling2D(name='pool_final')(x)\n",
    "x = Dense(n_classes, name='fc6')(x)\n",
    "output = Activation('softmax', name='prob')(x)\n",
    "model = Model(inputs=img_input, outputs=output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def weightedLoss(originalLossFunc, weightsList):\n",
    "\n",
    "    @tf.function\n",
    "    def lossFunc(true, pred):\n",
    "\n",
    "        axis = -1 #if channels last \n",
    "        #axis=  1 #if channels first\n",
    "\n",
    "        #argmax returns the index of the element with the greatest value\n",
    "        #done in the class axis, it returns the class index    \n",
    "        classSelectors = tf.argmax(true, axis=axis, output_type=tf.int32) \n",
    "\n",
    "        #considering weights are ordered by class, for each class\n",
    "        #true(1) if the class index is equal to the weight index   \n",
    "        classSelectors = [tf.equal(i, classSelectors) for i in range(len(weightsList))]\n",
    "\n",
    "        #casting boolean to float for calculations  \n",
    "        #each tensor in the list contains 1 where ground true class is equal to its index \n",
    "        #if you sum all these, you will get a tensor full of ones. \n",
    "        classSelectors = [tf.cast(x, tf.float32) for x in classSelectors]\n",
    "\n",
    "        #for each of the selections above, multiply their respective weight\n",
    "        weights = [sel * w for sel,w in zip(classSelectors, weightsList)] \n",
    "\n",
    "        #sums all the selections\n",
    "        #result is a tensor with the respective weight for each element in predictions\n",
    "        weightMultiplier = weights[0]\n",
    "        for i in range(1, len(weights)):\n",
    "            weightMultiplier = weightMultiplier + weights[i]\n",
    "\n",
    "\n",
    "        #make sure your originalLossFunc only collapses the class axis\n",
    "        #you need the other axes intact to multiply the weights tensor\n",
    "        loss = originalLossFunc(true,pred) \n",
    "        loss = loss * weightMultiplier\n",
    "\n",
    "        return loss\n",
    "    return lossFunc\n",
    "loss_object = weightedLoss(loss_object, class_weights)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0\n",
    "for artist in os.listdir(origin_dir + 'train/'):\n",
    "    train_size += len(glob.glob(origin_dir + 'train/' + artist +'/*.jpg'))\n",
    "        \n",
    "test_size = 0\n",
    "for artist in os.listdir(origin_dir + 'test/'):\n",
    "    test_size += len(glob.glob(origin_dir + 'test/' + artist +'/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7465 images belonging to 50 classes.\n",
      "Found 1690 images belonging to 50 classes.\n"
     ]
    }
   ],
   "source": [
    "# create summary writers\n",
    "train_summary_writer = tf.summary.create_file_writer('results/summaries/train/' + identifier)\n",
    "test_summary_writer = tf.summary.create_file_writer('results/summaries/test/' + identifier)\n",
    "\n",
    "# create data generators\n",
    "train_gen =  datagen.flow_from_directory(origin_dir + 'train/', target_size=size, batch_size=batch_size)\n",
    "test_gen = test_datagen.flow_from_directory(origin_dir + 'test/', target_size=size, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "min_loss = 100\n",
    "min_loss_acc = 0\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 01:41:09.364110 140673916643136 image.py:693] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_gen:\n",
    "    plt.imshow((images[0] - mean) / variance)\n",
    "    print(labels[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 01:41:13.952956 140673916643136 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 4.963510513305664, Train Acc:9.162759780883789, Test Loss: 4.776061534881592, Test Acc: 11.893491744995117, Time: 7605.044733285904 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0830 04:08:19.302898 140673916643136 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-18-2c4d58db66cd>\", line 8, in <module>\n",
      "    train_step(images, labels)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 404, in __call__\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1335, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 589, in _filtered_call\n",
      "    (t for t in nest.flatten((args, kwargs), expand_composites=True)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 671, in _call_flat\n",
      "    outputs = self._inference_function.call(ctx, args)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 445, in call\n",
      "    ctx=ctx)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 61, in quick_execute\n",
      "    num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1488, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1446, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/usr/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(\"starting training\")\n",
    "time_record = ''\n",
    "for epoch in range(epochs):\n",
    "    time_start = time.time()\n",
    "    batches = 0\n",
    "    for images, labels in train_gen:\n",
    "        images = (images - mean) / variance\n",
    "        train_step(images, labels)\n",
    "        batches += 1\n",
    "        if batches >= train_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "\n",
    "    batches = 0\n",
    "    all_predictions = np.array([]).reshape(0, n_classes)\n",
    "    all_labels = np.array([]).reshape(0, n_classes)\n",
    "    for test_images, test_labels in test_gen:\n",
    "        test_images = (test_images - mean) / variance\n",
    "        all_predictions = np.vstack((all_predictions, test_step(test_images, test_labels))) \n",
    "        all_labels = np.vstack((all_labels, test_labels))\n",
    "        batches += 1\n",
    "        if batches >= test_size / batch_size:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    time_finish = time.time()\n",
    "    end_time = (time_finish-time_start)\n",
    "    time_record = time_record + '{:.3f} s \\n'.format(end_time)\n",
    "\n",
    "    if (epoch % log_freq == 0):\n",
    "        print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}, Time: {} s'.format(\n",
    "               epoch,\n",
    "               train_loss.result(),\n",
    "               train_accuracy.result()*100,\n",
    "               test_loss.result(),\n",
    "               test_accuracy.result()*100,\n",
    "               end_time))\n",
    "\n",
    "        if (test_loss.result() < min_loss):    \n",
    "            if not os.path.exists(models_directory):\n",
    "                os.makedirs(models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(models_directory + \"best{}.h5\".format(identifier))\n",
    "            min_loss = test_loss.result()\n",
    "            min_loss_acc = test_accuracy.result()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "            #tf.summary.image('Confusion Matrix', image, step=epoch)\n",
    "            train_loss.reset_states()           \n",
    "            train_accuracy.reset_states()           \n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "            test_loss.reset_states()           \n",
    "            test_accuracy.reset_states()   \n",
    "            # save confusion matrix\n",
    "            con_mat = tf.math.confusion_matrix(\n",
    "                labels=np.argmax(all_labels, axis=1), \n",
    "                predictions=np.argmax(all_predictions, axis=1),\n",
    "                num_classes=n_classes).numpy()\n",
    "            con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "            con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                                 index = classes, \n",
    "                                 columns = classes)\n",
    "            figure = plt.figure(figsize=(8, 8))\n",
    "            sns.heatmap(con_mat_df, annot=False,cmap=plt.cm.Blues)\n",
    "            plt.tight_layout()\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            plt.close(figure)\n",
    "            buf.seek(0)\n",
    "            image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "            image = tf.expand_dims(image, 0)\n",
    "            tf.summary.image('Confusion Matrix', image, step=epoch)\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        break\n",
    "\n",
    "with open(os.path.join('results/', identifier), \"w\") as file1:\n",
    "    file1.write(time_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
